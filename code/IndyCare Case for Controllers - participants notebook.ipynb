{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# IndyCare Case: The NPS key drivers' analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Case introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Context\n",
    "<br>\n",
    "\n",
    "- IndyCare has started to gather **patient feedback** and data on the **Net Promotor Score (NPS)**\n",
    "- Management would like to have a better understanding of the **current deficiencies** in the hospitals and how these can be improved \n",
    "\n",
    "<center><img src=\"https://www.dropbox.com/s/2fagnil1d9hcdxq/charlie-nps-supporter.png?raw=1\" width=\"15%\" style=\"float:right\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Net Promotor Score (NPS)\n",
    "<br>\n",
    "\n",
    "- NPS is a leading indicator of **customer satisfaction**\n",
    "<img src=\"https://www.dropbox.com/s/pve2ojgz1h7qlgu/NPS.png?raw=1\" width=\"100%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM \n",
    "- The **CR**oss **I**ndustry **S**tandard **P**rocess for **D**ata **M**ining is a process model that serves as the base for a data science process with 6 sequential phases:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://www.dropbox.com/s/sojcporoik37epw/crispdm.png?raw=1\" width=\"50%\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Dataset IndyCare Case.csv').set_index('SN')\n",
    "df.head() # We take a look at the first 5 rows of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We observe that the data set does not contain any missing values, so we can proceed further with our Exploratory Data Analysis (EDA) stage which allows us to better understand our data. We now take a look at the distribution of our target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Goals of the case\n",
    "\n",
    "1. **Explore and visualize** the data to better understand its structure and check for anomalies;\n",
    "2. Build a **predictive model** to early identify the detractors and **evaluate** the model's performance;\n",
    "3. Determine which features have a **significant impact** on the NPS status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Split the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For this part of the case, we only look at the survey answers of the patients. Therefore, we remove all the patient characteristics from the dataset and store them in df_background. We will use this dataset in the following session when we cluster patients based on their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We create a new dataframe to put these background variables in\n",
    "df_background = df[[\"Department\", \"AgeYrs\", \"Gender\", \"MaritalStatus\",\"BedCategory\", \"Estimatedcost\"\n",
    "                  ,\"AdmissionDate\", \"DischargeDate\",\"LengthofStay\"]]\n",
    "\n",
    "# Remove the background variables from the \"df_survey\" dataset as well as the target variables NPS_Score and NPS_Status\n",
    "df_survey = df.drop(df_background.columns, axis=1)\n",
    "df_survey = df_survey.drop(['NPS_Score','NPS_Status'], axis = 1) \n",
    "df_target = df['NPS_Status'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This dataframe only contains the background variables\n",
    "df_background.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This dataframe only contains the answers from the survey questions\n",
    "# We will call these the 'features' of our model and represent the independent variables \n",
    "df_survey.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This dataframe only contains the target variable a.k.a. the dependent variable\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What type of predictive problem is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Target variable: Absolute and Relative Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Give the absolute count\n",
    "df_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Give the relative count\n",
    "df_target.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Can you make a plot that shows the count of the target classes?\n",
    "\n",
    "Hint: Ask ChatGPT to help you (use seaborn package)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(numeric_only=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What can you already infer from this plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explore the features  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Plot a histogram to visualize the distribution of at least one survey question in relation to the NPS_Status\n",
    "\n",
    "Hint: You can ask ChatGPT to help you plot the number of patients who gave a certain score in relation to their NPS Status\n",
    "\n",
    "Can you plot this for all survey questions in one code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Predictive modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary classification problem \n",
    "<br>\n",
    "\n",
    "- *Passives* don't generate bad word-of-mouth\n",
    "- Idea is to identify *detractors* early on and convert them into *promotors*\n",
    "- Focus on distinguising the *detractors* from the *passives* and *promotors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We create a new dataframe that contains the binary target\n",
    "df_binary = df.copy()\n",
    "# We then gather all the instances that are passives and promotors into \"non-detractors\"\n",
    "df_binary['NPS_Status'] = df['NPS_Status'].replace([\"Passive\", \"Promotor\"], \"Non-Detractor\")\n",
    "\n",
    "# We drop the background and target variables from our survey dataframe. It now only contains the features.\n",
    "df_survey_binary = df_binary.drop(df_background.columns, axis=1)\n",
    "df_survey_binary = df_survey_binary.drop(['NPS_Score','NPS_Status'], axis = 1) \n",
    "# We define our target dataframe\n",
    "df_target_binary = df_binary['NPS_Status'] \n",
    "\n",
    "df_target_binary.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We create a new dataframe for our binary target. We do however get an even more imbalanced dataset, so we will need to apply some resampling technique as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining our training and test variables \n",
    "The <a href=https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html>`train_test_split`</a> procedure involves taking a dataset and dividing it into two subsets. \n",
    "\n",
    "- Train Dataset: used to fit the machine learning model.\n",
    "- Test Dataset: used to evaluate the fit of the machine learning model and its subsequent predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Define the X and y variable(s) for our model for both the training and test set. Use the following parameters:\n",
    "- test_size = 0.2 \n",
    "- random_state = 42\n",
    "\n",
    "Hint: Look at the examples of <a href=https://www.geeksforgeeks.org/how-to-do-train-test-split-using-sklearn-in-python>`train_test_split`</a> or ask ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show the first few rows of X_train \n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Show the first few rows of y_train\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predictive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "###### Logistic regression\n",
    "    \n",
    "&#43; Easy to implement<br>\n",
    "&#43; High interpretation<br>\n",
    "&#8722; Lower performance<br>\n",
    "&#8722; Linear boundary\n",
    "\n",
    "<center><img src=\"https://www.dropbox.com/s/nfl722ylatv9sgo/LR_boundary.png?raw=1\" width=\"70%\" style=\"float:right\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert target variable to binary format\n",
    "y_train = np.where(y_train == 'Detractor', 1, 0)\n",
    "y_test = np.where(y_test == 'Detractor', 1, 0)\n",
    "target_names = ['Non-Detractors', 'Detractors']\n",
    "\n",
    "# Define a logistic regression classifier\n",
    "LR = LogisticRegression(solver='newton-cg', max_iter=500)\n",
    "# Fit the classifier on the training data\n",
    "LR.fit(X_train, y_train)\n",
    "y_pred= LR.predict(X_test)\n",
    "\n",
    "# Estimate the accuracy of the classifier on both training and test data\n",
    "print('Accuracy of LR classifier on overall sample training set: {:.2f}'\n",
    "     .format(LR.score(X_train, y_train)))\n",
    "print('Accuracy of LR classifier on overall sample test set: {:.2f}\\n'\n",
    "     .format(LR.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = target_names)\n",
    "disp.plot()\n",
    "plt.title('Logistic Regression \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Scoring metrics\n",
    "<br>\n",
    "\n",
    "- **precision**: proportion of predicted instances that truly belongs to that class $\\frac{tp}{tp+fp}$\n",
    "- **recall**: proportion of actual instances of a class correctly classified $\\frac{tp}{tp+fn}$\n",
    "- **f1-score**: mean between precision & recall\n",
    "- **support**  number of occurences of the given class in your test set \n",
    "\n",
    "&nbsp;\n",
    "- **accuracy**: proportion of correctly classified instances $\\frac{tp+tn}{tp+tn+fp+fn}$ \n",
    "- **macro average**: average, independent of distribution\n",
    "- **weighted average**: average, dependent on distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Are we happy with this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask ChatGPT to help you with the interpretation of the classification report.\n",
    "\n",
    "Hint: You can simply copy-paste the table in ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "&rarr; What can we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Treating the data imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Over-sample the minority (*passive* and *detractor*) classes using:<br>\n",
    "\n",
    "- **Resampling** where you duplicate observations from your minority class in your training set\n",
    "    - Simply adding duplicate records of minority class often donâ€™t add any new information to the model\n",
    "    \n",
    " \n",
    "- **SMOTE** where you synthesize new instances from the existing minority observations in your training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#we make sure that our training dataset is a balanced training dataset \n",
    "sm = SMOTE(random_state=0)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check the **new distribution**: equal amounts for the three classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Old distribution\n",
    "pd.Series(y_train).value_counts().rename({0: 'Non-Detractor', 1: 'Detractor'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# New distribution\n",
    "pd.Series(y_train_res).value_counts().rename({0: 'Non-Detractor', 1: 'Detractor'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 4\n",
    "Apply the logistic regression again and compare the results with those of the previous - imbalanced - classifier. <br>\n",
    "There is no need to convert the target variable to binary format like previously. This needs to be done only once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Feature importance: Key drivers of NPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Goal: Determine which <u>*features*</u> have a **significant impact** on our model's prediction of the NPS status\n",
    "- Provides insights into explainability/interpretability of the model\n",
    "- Can be used for feature selection to improve the final model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's look at the **feature importance** for our **Logistic Regression** model, as this model had the highest recall for *detractors* of all classifiers and is thus most suited in correctly differentiating *detractors* from the *non-detractors*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression: Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get importance\n",
    "importance = LR.coef_[0]\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "plt.barh(X_train.columns, importance)\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Convert y_train_res to binary format\n",
    "y_train_binary = np.where(y_train_res == 'Detractor', 0, 1)\n",
    "y_train_binary\n",
    "\n",
    "# Create and fit the logistic regression model\n",
    "logit_model = sm.Logit(y_train_binary, X_train_res)\n",
    "result = logit_model.fit(disp=False)\n",
    "\n",
    "# Get the coefficients, standard errors, and p-values\n",
    "coefficients = LR.coef_[0]\n",
    "std_errors = result.bse\n",
    "p_values = result.pvalues\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame({'Coefficients': coefficients,\n",
    "                           'Standard Errors': std_errors,\n",
    "                           'P-values': p_values})\n",
    "\n",
    "# Add stars to indicate significance levels\n",
    "alpha_10 = 0.10\n",
    "alpha_5 = 0.05\n",
    "alpha_1 = 0.01\n",
    "\n",
    "results_df['Significance'] = results_df['P-values'].apply(lambda x: '***' if x < alpha_1 else '**' if x < alpha_5 else '*' if x < alpha_10 else '')\n",
    "\n",
    "# Format the DataFrame\n",
    "styled_results_df = results_df.style\\\n",
    "    .format({'Coefficients': '{:.4f}',\n",
    "             'Standard Errors': '{:.4f}',\n",
    "             'P-values': '{:.4f}'})\\\n",
    "    .set_caption('Logistic Regression Results')\\\n",
    "    .set_table_attributes('class=\"dataframe\"')\n",
    "\n",
    "\n",
    "\n",
    "# Display the formatted DataFrame\n",
    "display(styled_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 5\n",
    "How should we interpret the above coefficients and significance levels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "rise": {
   "autolaunch": false,
   "enable_chalkboard": true,
   "overlay": "<img id='rise-backimage' src=vlerick.png /></div>",
   "scroll": true,
   "start_slideshow_at": "selected"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
